{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 09:23:58.293839: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-11 09:24:00.075009: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-11 09:24:00.075118: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-11 09:24:00.075128: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import VideoMAEImageProcessor, VideoMAEModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"MCG-NJU/videomae-base\"\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VideoMAEFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/videomae/feature_extraction_videomae.py:28: FutureWarning: The class VideoMAEFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use VideoMAEImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#dataset_root_path = '/data/ZHO/formats/ucf101_letters_only_avi/'\n",
    "dataset_root_path = '/projects/ZHO/formats/ucf101_letters_only/'\n",
    "\n",
    "feature_extractor = VideoMAEFeatureExtractor.from_pretrained(model_ckpt)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "dataset_root_path = pathlib.Path(dataset_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 32 32\n",
      "Total videos: 128\n"
     ]
    }
   ],
   "source": [
    "video_count_train = len(list(dataset_root_path.glob(\"train/*/*.mp4\")))\n",
    "video_count_val = len(list(dataset_root_path.glob(\"val/*/*.mp4\")))\n",
    "video_count_test = len(list(dataset_root_path.glob(\"test/*/*.mp4\")))\n",
    "video_total = video_count_train + video_count_val + video_count_test\n",
    "print(video_count_train, video_count_val, video_count_test)\n",
    "print(f\"Total videos: {video_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/projects/ZHO/formats/ucf101_letters_only/train/seen/Ganzo_seen.mp4'),\n",
       " PosixPath('/projects/ZHO/formats/ucf101_letters_only/train/seen/Original_seen.mp4'),\n",
       " PosixPath('/projects/ZHO/formats/ucf101_letters_only/train/noon/Ganzo_noon.mp4'),\n",
       " PosixPath('/projects/ZHO/formats/ucf101_letters_only/train/noon/Original_noon.mp4'),\n",
       " PosixPath('/projects/ZHO/formats/ucf101_letters_only/train/sheen/Original_sheen.mp4')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_video_file_paths = (\n",
    "    list(dataset_root_path.glob(\"train/*/*.mp4\"))\n",
    "    + list(dataset_root_path.glob(\"val/*/*.mp4\"))\n",
    "    + list(dataset_root_path.glob(\"test/*/*.mp4\"))\n",
    ")\n",
    "all_video_file_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: ['aeen', 'alif', 'baa', 'daad', 'daal', 'faa', 'gaen', 'haa', 'haa_1', 'hamza_wow', 'hamza_yaa', 'jeem', 'kaaf', 'khaa', 'laa', 'laam', 'meem', 'noon', 'qaaf', 'raa', 'saa', 'saad', 'seen', 'sheen', 'taa', 'taa_1', 'tua', 'wow', 'yaa', 'zaai', 'zaal', 'zua'].\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "class_labels = sorted({str(path).split(\"/\")[6] for path in all_video_file_paths})\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"Unique classes: {list(label2id.keys())}.\")\n",
    "print(len(id2label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorchvideo.data\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "mean = feature_extractor.image_mean\n",
    "std = feature_extractor.image_std\n",
    "resize_to = feature_extractor.size['shortest_edge']\n",
    "print(resize_to)\n",
    "num_frames_to_sample = 16#model.config.num_frames\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "#clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "clip_duration =10\n",
    "print(clip_duration)\n",
    "\n",
    "# Training dataset transformations.\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    #RandomShortSideScale(min_size=256, max_size=320),\n",
    "                    Resize((resize_to,resize_to)),\n",
    "                    #RandomHorizontalFlip(p=0.5),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# Training dataset.\n",
    "train_dataset = pytorchvideo.data.labeled_video_dataset(\n",
    "    data_path=os.path.join(dataset_root_path, \"train\"),\n",
    "    #data_path = os.path.join(dataset_root_path,'annotations','trainlist01.txt'),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=train_transform,\n",
    ")\n",
    "\n",
    "# Validation and evaluation datasets' transformations.\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize((resize_to, resize_to)),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "test_dataset = pytorchvideo.data.labeled_video_dataset(\n",
    "    data_path=os.path.join(dataset_root_path, \"test\"),\n",
    "    #data_path = os.path.join(dataset_root_path,'annotations','trainlist01.txt'),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_video = next(iter(train_dataset))\n",
    "#sample_video.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_video(sample_video):\n",
    "    \"\"\"Utility to investigate the keys present in a single video sample.\"\"\"\n",
    "    for k in sample_video:\n",
    "        if k == \"video\":\n",
    "            print(k, sample_video[\"video\"].shape)\n",
    "        else:\n",
    "            print(k, sample_video[k])\n",
    "\n",
    "    print(f\"Video label: {id2label[sample_video[k]]}\")\n",
    "\n",
    "\n",
    "#investigate_video(sample_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "def unnormalize_img(img):\n",
    "    \"\"\"Un-normalizes the image pixels.\"\"\"\n",
    "    img = (img * std) + mean\n",
    "    img = (img * 255).astype(\"uint8\")\n",
    "    return img.clip(0, 255)\n",
    "\n",
    "\n",
    "def create_gif(video_tensor, filename=\"sample.gif\"):\n",
    "    \"\"\"Prepares a GIF from a video tensor.\n",
    "    \n",
    "    The video tensor is expected to have the following shape:\n",
    "    (num_frames, num_channels, height, width).\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for video_frame in video_tensor:\n",
    "        frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n",
    "        frames.append(frame_unnormalized)\n",
    "    kargs = {\"duration\": 0.25}\n",
    "    imageio.mimsave(filename, frames, \"GIF\", **kargs)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def display_gif(video_tensor, gif_name=\"sample.gif\"):\n",
    "    \"\"\"Prepares and displays a GIF from a video tensor.\"\"\"\n",
    "    video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "    gif_filename = create_gif(video_tensor, gif_name)\n",
    "    return Image(filename=gif_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_video = next(iter(train_dataset))\n",
    "#video_tensor = sample_video[\"video\"]\n",
    "#display_gif(video_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19 28]\n",
      "[14 10]\n",
      "[ 7 11]\n",
      "[21  2]\n",
      "[ 0 31]\n",
      "[15 24]\n",
      "[31 22]\n",
      "[13  3]\n",
      "[11  0]\n",
      "[4 7]\n",
      "[14  4]\n",
      "[17 29]\n",
      "[12 25]\n",
      "[10 29]\n",
      "[6 9]\n",
      "[21 23]\n",
      "[20 20]\n",
      "[27  8]\n",
      "[24 16]\n",
      "[22 16]\n",
      "[5 2]\n",
      "[27  3]\n",
      "[30 23]\n",
      "[30  6]\n",
      "[ 1 15]\n",
      "[19  5]\n",
      "[ 8 26]\n",
      "[18 25]\n",
      "[28 13]\n",
      "[ 9 12]\n",
      "[18  1]\n",
      "[17 26]\n",
      "[24]\n",
      "[31]\n",
      "[17]\n",
      "[2]\n",
      "[3]\n",
      "[9]\n",
      "[23]\n",
      "[7]\n",
      "[20]\n",
      "[10]\n",
      "[6]\n",
      "[4]\n",
      "[22]\n",
      "[26]\n",
      "[11]\n",
      "[5]\n",
      "[18]\n",
      "[1]\n",
      "[14]\n",
      "[21]\n",
      "[25]\n",
      "[13]\n",
      "[19]\n",
      "[12]\n",
      "[27]\n",
      "[15]\n",
      "[16]\n",
      "[29]\n",
      "[8]\n",
      "[0]\n",
      "[30]\n",
      "[28]\n"
     ]
    }
   ],
   "source": [
    "#### Save processed videos and tensor\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=32,shuffle = False, num_workers=32)  \n",
    "test_dataloader = DataLoader(test_dataset,batch_size=32,num_workers=32)  \n",
    "\n",
    "big_tensor_video_train = np.zeros((64,3,16,224,224))\n",
    "#big_tensor_label_train = {}\n",
    "big_tensor_video_train_list = {k:[] for k in range(32)}\n",
    "\n",
    "big_tensor_video_test = np.zeros((32,3,16,224,224))\n",
    "#big_tensor_label_test = []\n",
    "\n",
    "\n",
    "for i,j in enumerate(train_dataloader):\n",
    "\n",
    "#    input = j['video'].permute(0,2,1,3,4)\n",
    "\n",
    "    input = j['video'].numpy()\n",
    "    labels = j['label'].numpy()\n",
    "\n",
    "    for k in range(input.shape[0]):\n",
    "\n",
    "        big_tensor_video_train_list[labels[k]].append(input[k])\n",
    "    \n",
    "    #big_tensor_video_train.append(input)    \n",
    "    print(labels)\n",
    "\n",
    "for k in range(len(big_tensor_video_train_list)):\n",
    "    for jj in range(2):\n",
    "        big_tensor_video_train[2*k+jj]=big_tensor_video_train_list[k][jj]\n",
    "\n",
    "for i,j in enumerate(test_dataloader):\n",
    "\n",
    "#    input = j['video'].permute(0,2,1,3,4)\n",
    "\n",
    "    input = j['video'].numpy()\n",
    "    labels = j['label'].numpy()\n",
    "\n",
    "    for k in range(input.shape[0]):\n",
    "\n",
    "        big_tensor_video_test[labels[k]] = input[k]\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_tensor_video_train_reshaped = np.transpose(big_tensor_video_train, (0,2,1,3,4))\n",
    "big_tensor_video_test_reshaped  = np.transpose(big_tensor_video_test, (0,2,1,3,4))\n",
    "\n",
    "np.save('train_video_letters_only_data.npy',big_tensor_video_train_reshaped)\n",
    "np.save('test_video_letters_only_data.npy',big_tensor_video_test_reshaped)\n",
    "\n",
    "\n",
    "\n",
    "#np.save('train_video_letters_only_data.npy',big_tensor_video_train)\n",
    "\n",
    "#np.save('test_video_letters_only_data.npy',big_tensor_video_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_tensor_video_train = np.load('train_video_letters_only_data.npy')\n",
    "big_tensor_video_test = np.load('test_video_letters_only_data.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 16, 3, 224, 224)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_tensor_video_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 16, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "big_tensor_video_test_1 = np.concatenate([big_tensor_video_train[a:a+1] for a in range(0,len(big_tensor_video_train),2)],axis=0)\n",
    "\n",
    "print(big_tensor_video_test_1.shape)\n",
    "big_tensor_video_train_1 = []\n",
    "\n",
    "for i in range(int(len(big_tensor_video_train)/2)):\n",
    "    big_tensor_video_train_1.append(big_tensor_video_train[i*2:i*2+1])\n",
    "    big_tensor_video_train_1.append(big_tensor_video_test[i:i+1])\n",
    "    \n",
    "big_tensor_video_train_1 = np.concatenate(big_tensor_video_train_1,axis =0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "big_tensor_video_test_2 = np.concatenate([big_tensor_video_train[a:a+1] for a in range(1,len(big_tensor_video_train),2)],axis =0)\n",
    "\n",
    "big_tensor_video_train_2 = []\n",
    "\n",
    "for i in range(int(len(big_tensor_video_train)/2)):\n",
    "    big_tensor_video_train_2.append(big_tensor_video_train[i*2+1:i*2+2])\n",
    "    big_tensor_video_train_2.append(big_tensor_video_test[i:i+1])\n",
    "    \n",
    "big_tensor_video_train_2 = np.concatenate(big_tensor_video_train_2,axis =0)\n",
    "\n",
    "\n",
    "big_tensor_video_train_2.shape\n",
    "\n",
    "np.save('train_video_letters_only_data_1.npy',big_tensor_video_train_1)\n",
    "\n",
    "np.save('test_video_letters_only_data_1.npy',big_tensor_video_test_1)\n",
    "\n",
    "np.save('train_video_letters_only_data_2.npy',big_tensor_video_train_2)\n",
    "\n",
    "np.save('test_video_letters_only_data_2.npy',big_tensor_video_test_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 16, 3, 224, 224)\n",
      "(32, 16, 3, 224, 224)\n",
      "(64, 16, 3, 224, 224)\n",
      "(64, 16, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "print(big_tensor_video_test_1.shape)\n",
    "print(big_tensor_video_test_2.shape)\n",
    "print(big_tensor_video_train_1.shape)\n",
    "print(big_tensor_video_train_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device='cuda'\n",
    "class CustomTensorDataset(Dataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].shape[0] == tensor.shape[0] for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[index]\n",
    "        #if self.transform:\n",
    "        #    x = self.transform(x)\n",
    "        \n",
    "        #z = torch.from_numpy(np.array(self.tensors[1][index]))\n",
    "        #z = self.tensors[1][index] \n",
    "        #print(index)\n",
    "        z = torch.Tensor([int(index/2)])\n",
    "        return x, z #torch.Tensor(int(index/2))\n",
    "    def __len__(self):\n",
    "        return self.tensors.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([int(4.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "device='cuda'\n",
    "class CustomTensorDatasetTrain(CustomTensorDataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, n_class =32, transform=None):\n",
    "        super().__init__(tensors,transform)\n",
    "        self.class_arange = np.arange(n_class)\n",
    "        #self.order = []\n",
    "        self.order= [int(i/2) for i in range(n_class*2)]\n",
    "        #print(self.order)\n",
    "        #self.finished_flag = False\n",
    "    def __getitem__(self, index):\n",
    "        #x = self.tensors[index]\n",
    "        #if self.transform:\n",
    "        #    x = self.transform(x)\n",
    "\n",
    "        print('index_train:',index)\n",
    "        #print('index_train:',self.order[index])\n",
    "        x = self.tensors[self.order[index]]\n",
    "        z = torch.Tensor([self.order[index]])\n",
    "        \n",
    "        return x,z\n",
    "    def __len__(self):\n",
    "        return self.tensors.shape[0]\n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self.class_arange)\n",
    "        self.order = []\n",
    "        for i in self.class_arange:\n",
    "            self.order = self.order + [i,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21, 22, 22, 23, 23, 24, 24, 25, 25, 26, 26, 27, 27, 28, 28, 29, 29, 30, 30, 31, 31]\n"
     ]
    }
   ],
   "source": [
    "order = []\n",
    "class_arange = np.arange(32)\n",
    "for i in class_arange:\n",
    "    order = order + [i,i]\n",
    "print(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomTensorDatasetTrain(big_tensor_video_train)\n",
    "#test_dataset = CustomTensorDataset(big_tensor_video_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=16,shuffle = False, num_workers=1)  \n",
    "test_dataloader = DataLoader(test_dataset,batch_size = 16)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "index_train: 0\n",
      "index_train: 1\n",
      "index_train: 2\n",
      "index_train: 3\n",
      "index_train: 4\n",
      "index_train: 5\n",
      "index_train: 6\n",
      "index_train: 7\n",
      "index_train: 8\n",
      "index_train: 9\n",
      "index_train: 10\n",
      "index_train: 11\n",
      "index_train: 12\n",
      "index_train: 13\n",
      "index_train: 14\n",
      "index_train: 15\n",
      "index_train: 16\n",
      "index_train: 17\n",
      " index_train: 18\n",
      "index_train: 19\n",
      "index_train:20\n",
      "index_train: 21\n",
      "index_train: 22\n",
      "index_train: 23\n",
      "index_train: 24\n",
      "index_train: 25\n",
      "index_train: 26\n",
      "index_train: 27\n",
      "index_train: 28\n",
      "index_train: 29\n",
      "index_train: 30\n",
      "index_train: 31\n",
      "y: tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [7.]])\n",
      "index_train: 32\n",
      "index_train: 33\n",
      "index_train:index_train: 34\n",
      "index_train: 35\n",
      "index_train: 36\n",
      "index_train: 37\n",
      "index_train: 38\n",
      "index_train: 39\n",
      "index_train: 40\n",
      "index_train: 41\n",
      "index_train: 42\n",
      "index_train: 43\n",
      " 44\n",
      "index_train: 45\n",
      "index_train: 46\n",
      "index_train: 47\n",
      "y: tensor([[ 8.],\n",
      "        [ 8.],\n",
      "        [ 9.],\n",
      "        [ 9.],\n",
      "        [10.],\n",
      "        [10.],\n",
      "        [11.],\n",
      "        [11.],\n",
      "        [12.],\n",
      "        [12.],\n",
      "        [13.],\n",
      "        [13.],\n",
      "        [14.],\n",
      "        [14.],\n",
      "        [15.],\n",
      "        [15.]])\n",
      "index_train: 48\n",
      "index_train:  49\n",
      "index_train: 50\n",
      "index_train: 51\n",
      "index_train: 52\n",
      "index_train: 53\n",
      "index_train: 54\n",
      "index_train: 55\n",
      "index_train: 56\n",
      "index_train: 57\n",
      "index_train: 58\n",
      "index_train:59\n",
      "index_train: 60\n",
      "index_train: 61\n",
      "index_train: 62\n",
      "index_train: 63\n",
      "y: tensor([[16.],\n",
      "        [16.],\n",
      "        [17.],\n",
      "        [17.],\n",
      "        [18.],\n",
      "        [18.],\n",
      "        [19.],\n",
      "        [19.],\n",
      "        [20.],\n",
      "        [20.],\n",
      "        [21.],\n",
      "        [21.],\n",
      "        [22.],\n",
      "        [22.],\n",
      "        [23.],\n",
      "        [23.]])\n",
      "y: tensor([[24.],\n",
      "        [24.],\n",
      "        [25.],\n",
      "        [25.],\n",
      "        [26.],\n",
      "        [26.],\n",
      "        [27.],\n",
      "        [27.],\n",
      "        [28.],\n",
      "        [28.],\n",
      "        [29.],\n",
      "        [29.],\n",
      "        [30.],\n",
      "        [30.],\n",
      "        [31.],\n",
      "        [31.]])\n",
      "epoch: 1\n",
      "index_train: 0\n",
      "index_train: 1\n",
      "index_train: 2\n",
      "index_train: 3\n",
      "index_train: 4\n",
      "index_train: 5\n",
      "index_train: 6\n",
      "index_train: 7\n",
      "index_train: 8\n",
      "index_train: 9\n",
      "index_train: 10\n",
      "index_train: 11\n",
      "index_train: 12\n",
      "index_train: 13\n",
      "index_train: 14\n",
      "index_train: 15\n",
      "index_train: 16\n",
      "index_train: 17\n",
      "index_train: 18\n",
      "index_train: 19\n",
      "index_train: 20\n",
      "index_train: 21\n",
      "index_train: 22\n",
      "index_train: 23\n",
      "index_train: 24\n",
      "index_train: 25\n",
      "index_train: 26\n",
      "index_train: 27\n",
      "index_train: 28\n",
      "index_train: 29\n",
      "index_train: 30\n",
      "index_train: 31\n",
      "y: tensor([[18.],\n",
      "        [18.],\n",
      "        [ 9.],\n",
      "        [ 9.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [27.],\n",
      "        [27.],\n",
      "        [17.],\n",
      "        [17.],\n",
      "        [30.],\n",
      "        [30.],\n",
      "        [19.],\n",
      "        [19.],\n",
      "        [ 5.],\n",
      "        [ 5.]])\n",
      "index_train: 32\n",
      "index_train: 4433\n",
      "index_train: 34\n",
      "index_train: 35\n",
      "index_train: 36\n",
      "index_train: 37\n",
      "index_train: 38\n",
      "index_train: 39\n",
      "index_train: 40\n",
      "index_train: 41\n",
      "index_train: 42\n",
      "index_train: 43\n",
      "index_train: \n",
      "index_train: 45\n",
      "index_train: 46\n",
      "index_train: 47\n",
      "y: tensor([[28.],\n",
      "        [28.],\n",
      "        [ 0.],\n",
      "        [ 0.],\n",
      "        [ 2.],\n",
      "        [ 2.],\n",
      "        [12.],\n",
      "        [12.],\n",
      "        [16.],\n",
      "        [16.],\n",
      "        [14.],\n",
      "        [14.],\n",
      "        [15.],\n",
      "        [15.],\n",
      "        [29.],\n",
      "        [29.]])\n",
      "index_train: 48\n",
      "index_train: 5949\n",
      "index_train: 50\n",
      "index_train: 51\n",
      "index_train: 52\n",
      "index_train: 53\n",
      "index_train: 54\n",
      "index_train: 55\n",
      "index_train: 56\n",
      "index_train: 57\n",
      "index_train: 58\n",
      "index_train: \n",
      "index_train: 60\n",
      "index_train: 61\n",
      "index_train: 62\n",
      "index_train: 63\n",
      "y: tensor([[21.],\n",
      "        [21.],\n",
      "        [ 7.],\n",
      "        [ 7.],\n",
      "        [11.],\n",
      "        [11.],\n",
      "        [ 4.],\n",
      "        [ 4.],\n",
      "        [26.],\n",
      "        [26.],\n",
      "        [23.],\n",
      "        [23.],\n",
      "        [24.],\n",
      "        [24.],\n",
      "        [13.],\n",
      "        [13.]])\n",
      "y: tensor([[25.],\n",
      "        [25.],\n",
      "        [31.],\n",
      "        [31.],\n",
      "        [ 3.],\n",
      "        [ 3.],\n",
      "        [ 8.],\n",
      "        [ 8.],\n",
      "        [20.],\n",
      "        [20.],\n",
      "        [ 6.],\n",
      "        [ 6.],\n",
      "        [10.],\n",
      "        [10.],\n",
      "        [22.],\n",
      "        [22.]])\n",
      "epoch: 2\n",
      "index_train: 0\n",
      "index_train: 1\n",
      "index_train: 2\n",
      "index_train: 3\n",
      "index_train: 4\n",
      "index_train: 5\n",
      "index_train: 6\n",
      "index_train: 7\n",
      "index_train: 8\n",
      "index_train: 9\n",
      "index_train: 10\n",
      "index_train: 11\n",
      "index_train: 12\n",
      "index_train: 13\n",
      "index_train: 14\n",
      "index_train: 15\n",
      "index_train: 16\n",
      "index_train: 17\n",
      "index_train: 18\n",
      "index_train: 19\n",
      "index_train: 20\n",
      "index_train: 21\n",
      "index_train: 22\n",
      "index_train: 23\n",
      "index_train: 24\n",
      "index_train: 25\n",
      "index_train: 26\n",
      "index_train: 27\n",
      "index_train: 28\n",
      "index_train: 29\n",
      "index_train: 30\n",
      "index_train: 31\n",
      "y: tensor([[ 6.],\n",
      "        [ 6.],\n",
      "        [17.],\n",
      "        [17.],\n",
      "        [19.],\n",
      "        [19.],\n",
      "        [ 0.],\n",
      "        [ 0.],\n",
      "        [10.],\n",
      "        [10.],\n",
      "        [29.],\n",
      "        [29.],\n",
      "        [27.],\n",
      "        [27.],\n",
      "        [ 3.],\n",
      "        [ 3.]])\n",
      "index_train: 32\n",
      "index_train:\n",
      " 33\n",
      "index_train: 34\n",
      "index_train: 35\n",
      "index_train: 36\n",
      "index_train: 37\n",
      "index_train: 38\n",
      "index_train: 39\n",
      "index_train: 40\n",
      "index_train: 41\n",
      "index_train: 42\n",
      "index_train: 43index_train: 44\n",
      "index_train: 45\n",
      "index_train: 46\n",
      "index_train: 47\n",
      "y: tensor([[20.],\n",
      "        [20.],\n",
      "        [ 8.],\n",
      "        [ 8.],\n",
      "        [31.],\n",
      "        [31.],\n",
      "        [ 5.],\n",
      "        [ 5.],\n",
      "        [14.],\n",
      "        [14.],\n",
      "        [13.],\n",
      "        [13.],\n",
      "        [22.],\n",
      "        [22.],\n",
      "        [11.],\n",
      "        [11.]])\n",
      "index_train: 48\n",
      "index_train:  49\n",
      "index_train: 50\n",
      "index_train: 51\n",
      "index_train: 52\n",
      "index_train: 53\n",
      "index_train: 54\n",
      "index_train: 55\n",
      "index_train: 56\n",
      "index_train: 57\n",
      "index_train: 58\n",
      "index_train:59\n",
      "index_train: 60\n",
      "index_train: 61\n",
      "index_train: 62\n",
      "index_train: 63\n",
      "y: tensor([[30.],\n",
      "        [30.],\n",
      "        [12.],\n",
      "        [12.],\n",
      "        [26.],\n",
      "        [26.],\n",
      "        [25.],\n",
      "        [25.],\n",
      "        [21.],\n",
      "        [21.],\n",
      "        [ 2.],\n",
      "        [ 2.],\n",
      "        [ 9.],\n",
      "        [ 9.],\n",
      "        [ 4.],\n",
      "        [ 4.]])\n",
      "y: tensor([[15.],\n",
      "        [15.],\n",
      "        [ 7.],\n",
      "        [ 7.],\n",
      "        [18.],\n",
      "        [18.],\n",
      "        [28.],\n",
      "        [28.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [24.],\n",
      "        [24.],\n",
      "        [23.],\n",
      "        [23.],\n",
      "        [16.],\n",
      "        [16.]])\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print('epoch:',epoch)    \n",
    "    for x,y in train_dataloader:\n",
    "        \n",
    "        print('y:',y)\n",
    "\n",
    "    train_dataset.shuffle()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,k in test_dataloader:\n",
    "    pass\n",
    "    #print('k:',k)\n",
    "#for i,j in enumerate(train_dataloader):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'training':CustomTensorDataset((dataa_x_train_8, dataa_y_train),transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
