{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "WARNING:root:Failed to import geometry msgs in rigid_transformations.py.\n",
      "WARNING:root:Failed to import ros dependencies in rigid_transforms.py\n",
      "WARNING:root:autolab_core not installed as catkin package, RigidTransform ros methods will be unavailable\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'VideoVisualizer' from 'visualization' (/usr/local/lib/python3.8/dist-packages/visualization/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorchvideo\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mava\u001b[39;00m \u001b[39mimport\u001b[39;00m AvaLabeledVideoFramePaths\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorchvideo\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhub\u001b[39;00m \u001b[39mimport\u001b[39;00m slow_r50_detection \u001b[39m# Another option is slowfast_r50_detection\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvisualization\u001b[39;00m \u001b[39mimport\u001b[39;00m VideoVisualizer\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'VideoVisualizer' from 'visualization' (/usr/local/lib/python3.8/dist-packages/visualization/__init__.py)"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "import detectron2\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "\n",
    "import pytorchvideo\n",
    "from pytorchvideo.transforms.functional import (\n",
    "    uniform_temporal_subsample,\n",
    "    short_side_scale_with_boxes,\n",
    "    clip_boxes_to_image,\n",
    ")\n",
    "from torchvision.transforms._functional_video import normalize\n",
    "from pytorchvideo.data.ava import AvaLabeledVideoFramePaths\n",
    "from pytorchvideo.models.hub import slow_r50_detection # Another option is slowfast_r50_detection\n",
    "\n",
    "from visualization import VideoVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' # or 'cpu'\n",
    "video_model = slow_r50_detection(True) # Another option is slowfast_r50_detection\n",
    "video_model = video_model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_final_280758.pkl: 167MB [00:09, 18.4MB/s]                              \n"
     ]
    }
   ],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.55  # set threshold for this model\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# This method takes in an image and generates the bounding boxes for people in the image.\n",
    "def get_person_bboxes(inp_img, predictor):\n",
    "    predictions = predictor(inp_img.cpu().detach().numpy())['instances'].to('cpu')\n",
    "    boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n",
    "    scores = predictions.scores if predictions.has(\"scores\") else None\n",
    "    classes = np.array(predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None)\n",
    "    predicted_boxes = boxes[np.logical_and(classes==0, scores>0.75 )].tensor.cpu() # only person\n",
    "    return predicted_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ava_inference_transform(\n",
    "    clip,\n",
    "    boxes,\n",
    "    num_frames = 4, #if using slowfast_r50_detection, change this to 32\n",
    "    crop_size = 256,\n",
    "    data_mean = [0.45, 0.45, 0.45],\n",
    "    data_std = [0.225, 0.225, 0.225],\n",
    "    slow_fast_alpha = None, #if using slowfast_r50_detection, change this to 4\n",
    "):\n",
    "\n",
    "    boxes = np.array(boxes)\n",
    "    ori_boxes = boxes.copy()\n",
    "\n",
    "    # Image [0, 255] -> [0, 1].\n",
    "    clip = uniform_temporal_subsample(clip, num_frames)\n",
    "    clip = clip.float()\n",
    "    clip = clip / 255.0\n",
    "\n",
    "    height, width = clip.shape[2], clip.shape[3]\n",
    "    # The format of boxes is [x1, y1, x2, y2]. The input boxes are in the\n",
    "    # range of [0, width] for x and [0,height] for y\n",
    "    boxes = clip_boxes_to_image(boxes, height, width)\n",
    "\n",
    "    # Resize short side to crop_size. Non-local and STRG uses 256.\n",
    "    clip, boxes = short_side_scale_with_boxes(\n",
    "        clip,\n",
    "        size=crop_size,\n",
    "        boxes=boxes,\n",
    "    )\n",
    "\n",
    "    # Normalize images by mean and std.\n",
    "    clip = normalize(\n",
    "        clip,\n",
    "        np.array(data_mean, dtype=np.float32),\n",
    "        np.array(data_std, dtype=np.float32),\n",
    "    )\n",
    "\n",
    "    boxes = clip_boxes_to_image(\n",
    "        boxes, clip.shape[2],  clip.shape[3]\n",
    "    )\n",
    "\n",
    "    # Incase of slowfast, generate both pathways\n",
    "    if slow_fast_alpha is not None:\n",
    "        fast_pathway = clip\n",
    "        # Perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(\n",
    "            clip,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0, clip.shape[1] - 1, clip.shape[1] // slow_fast_alpha\n",
    "            ).long(),\n",
    "        )\n",
    "        clip = [slow_pathway, fast_pathway]\n",
    "\n",
    "    return clip, torch.from_numpy(boxes), ori_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-25 13:08:13--  https://dl.fbaipublicfiles.com/pytorchvideo/data/class_names/ava_action_list.pbtxt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2649 (2.6K) [text/plain]\n",
      "Saving to: ‘ava_action_list.pbtxt.1’\n",
      "\n",
      "ava_action_list.pbt 100%[===================>]   2.59K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-11-25 13:08:14 (16.4 MB/s) - ‘ava_action_list.pbtxt.1’ saved [2649/2649]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dowload the action text to id mapping\n",
    "!wget https://dl.fbaipublicfiles.com/pytorchvideo/data/class_names/ava_action_list.pbtxt\n",
    "\n",
    "# Create an id to label name mapping\n",
    "label_map, allowed_class_ids = AvaLabeledVideoFramePaths.read_label_map('ava_action_list.pbtxt')\n",
    "# Create a video visualizer that can plot bounding boxes and visualize actions on bboxes.\n",
    "#video_visualizer = VideoVisualizer(81, label_map, top_k=3, mode=\"thres\",thres=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-25 13:08:18--  https://dl.fbaipublicfiles.com/pytorchvideo/projects/theatre.webm\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 36564580 (35M) [video/webm]\n",
      "Saving to: ‘theatre.webm’\n",
      "\n",
      "theatre.webm        100%[===================>]  34.87M  9.94MB/s    in 3.5s    \n",
      "\n",
      "2022-11-25 13:08:23 (9.94 MB/s) - ‘theatre.webm’ saved [36564580/36564580]\n",
      "\n",
      "Completed loading encoded video.\n"
     ]
    }
   ],
   "source": [
    "# Download the demo video.\n",
    "!wget https://dl.fbaipublicfiles.com/pytorchvideo/projects/theatre.webm\n",
    "\n",
    "# Load the video\n",
    "encoded_vid = pytorchvideo.data.encoded_video.EncodedVideo.from_path('theatre.webm')\n",
    "print('Completed loading encoded video.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for time stamp: 90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/detectron2/structures/boxes.py:235: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  b = self.tensor[item]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for time stamp: 91 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/detectron2/structures/boxes.py:235: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  b = self.tensor[item]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for time stamp: 92 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/detectron2/structures/boxes.py:235: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  b = self.tensor[item]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for time stamp: 93 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/detectron2/structures/boxes.py:235: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  b = self.tensor[item]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for time stamp: 94 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/detectron2/structures/boxes.py:235: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  b = self.tensor[item]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for time stamp: 95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/detectron2/structures/boxes.py:235: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  b = self.tensor[item]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for time stamp: 96 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/detectron2/structures/boxes.py:235: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  b = self.tensor[item]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for time stamp: 97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/detectron2/structures/boxes.py:235: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  b = self.tensor[item]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for time stamp: 98 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/detectron2/structures/boxes.py:235: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  b = self.tensor[item]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for time stamp: 99 sec\n",
      "Finished generating predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/detectron2/structures/boxes.py:235: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  b = self.tensor[item]\n"
     ]
    }
   ],
   "source": [
    "# Video predictions are generated at an internal of 1 sec from 90 seconds to 100 seconds in the video.\n",
    "time_stamp_range = range(90,100) # time stamps in video for which clip is sampled.\n",
    "clip_duration = 1.0 # Duration of clip used for each inference step.\n",
    "gif_imgs = []\n",
    "\n",
    "for time_stamp in time_stamp_range:\n",
    "    print(\"Generating predictions for time stamp: {} sec\".format(time_stamp))\n",
    "\n",
    "    # Generate clip around the designated time stamps\n",
    "    inp_imgs = encoded_vid.get_clip(\n",
    "        time_stamp - clip_duration/2.0, # start second\n",
    "        time_stamp + clip_duration/2.0  # end second\n",
    "    )\n",
    "    inp_imgs = inp_imgs['video']\n",
    "\n",
    "    # Generate people bbox predictions using Detectron2's off the self pre-trained predictor\n",
    "    # We use the the middle image in each clip to generate the bounding boxes.\n",
    "    inp_img = inp_imgs[:,inp_imgs.shape[1]//2,:,:]\n",
    "    inp_img = inp_img.permute(1,2,0)\n",
    "\n",
    "    # Predicted boxes are of the form List[(x_1, y_1, x_2, y_2)]\n",
    "    predicted_boxes = get_person_bboxes(inp_img, predictor)\n",
    "    if len(predicted_boxes) == 0:\n",
    "        print(\"Skipping clip no frames detected at time stamp: \", time_stamp)\n",
    "        continue\n",
    "\n",
    "    # Preprocess clip and bounding boxes for video action recognition.\n",
    "    inputs, inp_boxes, _ = ava_inference_transform(inp_imgs, predicted_boxes.numpy())\n",
    "    # Prepend data sample id for each bounding box.\n",
    "    # For more details refere to the RoIAlign in Detectron2\n",
    "    inp_boxes = torch.cat([torch.zeros(inp_boxes.shape[0],1), inp_boxes], dim=1)\n",
    "\n",
    "    # Generate actions predictions for the bounding boxes in the clip.\n",
    "    # The model here takes in the pre-processed video clip and the detected bounding boxes.\n",
    "    preds = video_model(inputs.unsqueeze(0).to(device), inp_boxes.to(device))\n",
    "\n",
    "\n",
    "    preds= preds.to('cpu')\n",
    "    # The model is trained on AVA and AVA labels are 1 indexed so, prepend 0 to convert to 0 index.\n",
    "    preds = torch.cat([torch.zeros(preds.shape[0],1), preds], dim=1)\n",
    "\n",
    "    # Plot predictions on the video and save for later visualization.\n",
    "    inp_imgs = inp_imgs.permute(1,2,3,0)\n",
    "    inp_imgs = inp_imgs/255.0\n",
    "    #out_img_pred = video_visualizer.draw_clip_range(inp_imgs, preds, predicted_boxes)\n",
    "    #gif_imgs += out_img_pred\n",
    "\n",
    "print(\"Finished generating predictions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m height, width \u001b[39m=\u001b[39m gif_imgs[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], gif_imgs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m      3\u001b[0m vide_save_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39moutput.mp4\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      4\u001b[0m video \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mVideoWriter(vide_save_path,cv2\u001b[39m.\u001b[39mVideoWriter_fourcc(\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDIVX\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m7\u001b[39m, (width,height))\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "height, width = gif_imgs[0].shape[0], gif_imgs[0].shape[1]\n",
    "\n",
    "vide_save_path = 'output.mp4'\n",
    "video = cv2.VideoWriter(vide_save_path,cv2.VideoWriter_fourcc(*'DIVX'), 7, (width,height))\n",
    "\n",
    "for image in gif_imgs:\n",
    "    img = (255*image).astype(np.uint8)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    video.write(img)\n",
    "video.release()\n",
    "\n",
    "print('Predictions are saved to the video file: ', vide_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
