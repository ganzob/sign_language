{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import matplotlib.pyplot as plt    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb43632b630>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(images, n_patches):\n",
    "    n, c, h, w = images.shape\n",
    "\n",
    "    assert h == w, \"Patchify method is implemented for square images only\"\n",
    "\n",
    "    patches = torch.zeros(n, n_patches **2 , h*w // n_patches **2)\n",
    "    patch_size = h // n_patches\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        for i in range(n_patches):\n",
    "            for j in range(n_patches):\n",
    "                patch = image[:, i * patch_size: (i+1) * patch_size, j * patch_size: (j+1) * patch_size]\n",
    "                patches[idx, i * n_patches + j] = patch.flatten()\n",
    "\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d, dtype=torch.double)\n",
    "    \n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i/(10000 ** (j/d))) if j%2 ==0 else np.cos(i/(10000 ** ((j-1)/d)))\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_positional_embeddings(30,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    plt.imshow(get_positional_embeddings(100,300), cmap = \"hot\", interpolation = \"nearest\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMSA(nn.Module):\n",
    "    def __init__(self, d, n_heads = 2):\n",
    "        super(MyMSA,self).__init__()\n",
    "        self.d = d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert d % n_heads == 0, f\"Can't divide dimension{d} into {n_heads} heads\"\n",
    "\n",
    "        d_head = int(d/ n_heads)\n",
    "\n",
    "        self.q_mappings = nn.ModuleList([nn.Linear(d_head,d_head) for _ in range(self.n_heads)])\n",
    "        self.k_mappings = nn.ModuleList([nn.Linear(d_head,d_head) for _ in range(self.n_heads)])\n",
    "        self.v_mappings = nn.ModuleList([nn.Linear(d_head,d_head) for _ in range(self.n_heads)])\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "\n",
    "                seq = sequence[:, head* self.d_head: (head+1) * self.d_head]\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / (self.d_head **0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "\n",
    "        return torch.cat([torch.unsqueeze(r , dim =0) for r in result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myViTBlock(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio = 4):\n",
    "        super(myViTBlock,self).__init__()\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        self.mhsa = MyMSA(hidden_d, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio* hidden_d, hidden_d)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print('x:',x.dtype,x.shape)\n",
    "        out =x + self.mhsa(self.norm1(x))\n",
    "        out = out + self.mlp(self.norm2(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViT(nn.Module):\n",
    "    def __init__(self, chw =(1,28,28), n_patches = 7, n_blocks =2, hidden_d =8, n_heads =2, out_d =10):\n",
    "        super(MyViT, self).__init__()\n",
    "\n",
    "        self.chw = chw\n",
    "        self.n_patches = n_patches\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_d = hidden_d\n",
    "        assert chw[1] % n_patches ==0, \"Input shape not entirely divisible by number of patches\"\n",
    "        assert chw[2] % n_patches ==0, \"Input shape not entirely divisible by number of patches\"\n",
    "\n",
    "        self.patch_size = (chw[1]/ n_patches, chw[2]/ n_patches)\n",
    "\n",
    "        # 1) Linear mapper\n",
    "\n",
    "        self.input_d = int(chw[0] * self.patch_size[0]* self.patch_size[1])\n",
    "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "\n",
    "        # 2) Learnable classification token\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "\n",
    "        # 3) Positional embedding\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(self.n_patches ** 2 +1,self.hidden_d)))\n",
    "        self.pos_embed.requires_grad = False\n",
    "\n",
    "        # 4) Transformer encoder blocks\n",
    "        self.blocks = nn.ModuleList([myViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n",
    "\n",
    "        # 5) Classification MLP\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_d,out_d),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        n, c, h, w = images.shape\n",
    "        patches = patchify(images, self.n_patches).to('cuda')\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        tokens = torch.stack([torch.vstack((self.class_token,tokens[i])) for i in range(len(tokens))])\n",
    "\n",
    "        pos_embed = self.pos_embed.repeat(n,1,1)\n",
    "        out = tokens + pos_embed\n",
    "\n",
    "        # Transformer Blocks\n",
    "        #print('ssLL:',out.shape)\n",
    "        for block in self.blocks:\n",
    "            #print('jj:',block)\n",
    "            out = block(out)\n",
    "\n",
    "        out = out[:,0]\n",
    "\n",
    "        return self.mlp(out)\n",
    "        #pass\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50199/3931125494.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(self.n_patches ** 2 +1,self.hidden_d)))\n"
     ]
    }
   ],
   "source": [
    " if __name__ == '__main__':\n",
    "\n",
    "      #model = myViTBlock(hidden_d = 8, n_heads = 2)\n",
    "    model = MyViT(chw = (1, 28, 28), n_patches = 7, n_blocks = 2,    hidden_d = 8,    n_heads =2,    out_d = 10)\n",
    "\n",
    "    model.cuda().float()\n",
    "    x = torch.randn(1,1,28,28, device='cuda')#.cuda()\n",
    "    #print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    transform = ToTensor()\n",
    "\n",
    "    train_set = MNIST(root='./datasets', train=True, download=False,transform=transform)\n",
    "    test_set = MNIST(root='./datasets', train = False, download=False, transform = transform)\n",
    "\n",
    "    train_loader = DataLoader(train_set, shuffle = True, batch_size =8192)\n",
    "    test_loader = DataLoader(test_set, shuffle= False, batch_size =16384)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('device:',device)\n",
    "    model = MyViT(\n",
    "    chw = (1, 28, 28),\n",
    "    n_patches = 7,\n",
    "    n_blocks = 2,\n",
    "    hidden_d = 8,\n",
    "    n_heads =2,\n",
    "    out_d = 10\n",
    "    )\n",
    "    model.to(device).float()\n",
    "\n",
    "    N_EPOCHS= 5\n",
    "\n",
    "    LR =0.005\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr = LR)\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    for epoch in tqdm(range(N_EPOCHS), desc = \"Training\"):\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc = f\"Epoch {epoch + 1} in training\", leave = True):\n",
    "            x, y = batch\n",
    "            \n",
    "#            x,y =x.double(), y.double()\n",
    "            x, y  = x.to(device), y.to(device)\n",
    "            #x, y = x.double().to(device), y.double().to(device)\n",
    "            #print('x:',x.dtype,x.shape)\n",
    "\n",
    "            #print('dd:',x.get_device())\n",
    "            #print(model.device())\n",
    "            print('x:',x.shape)\n",
    "            y_hat = model(x)\n",
    "\n",
    "            loss = criterion(y_hat,y)\n",
    "\n",
    "            train_loss += loss.detach().cpu().item()/ len(train_loader)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{N_EPOCHS} loss:{train_loss:.2f}\")\n",
    "# Test loop\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0,0\n",
    "        test_loss =0.0\n",
    "        for batch in tqdm(test_loader, desc = \"Testing\"):\n",
    "            x,y = batch\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat,y)\n",
    "            test_loss += loss.detach().cpu().item()/len(test_loader)\n",
    "\n",
    "            correct += torch.sum(torch.argmax(y_hat,dim =1) ==y ).detach().cpu().item()\n",
    "            total += len(x)\n",
    "\n",
    "        print(f\"Test loss:{ test_loss:.2f}\")\n",
    "        print(f\"Test accuracy: {correct / total *100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50199/3931125494.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(self.n_patches ** 2 +1,self.hidden_d)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8192, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8192, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8192, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8192, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8192, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8192, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8192, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([2656, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 in training: 100%|██████████| 8/8 [06:51<00:00, 51.38s/it]\n",
      "Training:  20%|██        | 1/5 [06:51<27:24, 411.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 loss:2.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8192, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8192, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8192, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8192, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8192, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8192, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8192, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([2656, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 in training: 100%|██████████| 8/8 [06:48<00:00, 51.07s/it]\n",
      "Training:  40%|████      | 2/5 [13:39<20:28, 409.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 loss:2.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8192, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8192, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8192, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8192, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8192, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 in training:  50%|█████     | 4/8 [03:45<03:45, 56.34s/it]\n",
      "Training:  40%|████      | 2/5 [17:25<26:07, 522.50s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m    main()\n",
      "Cell \u001b[0;32mIn [12], line 43\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39m#x, y = x.double().to(device), y.double().to(device)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m#print('x:',x.dtype,x.shape)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[39m#print('dd:',x.get_device())\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m#print(model.device())\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mx:\u001b[39m\u001b[39m'\u001b[39m,x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 43\u001b[0m y_hat \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     45\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_hat,y)\n\u001b[1;32m     47\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mitem()\u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [8], line 40\u001b[0m, in \u001b[0;36mMyViT.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, images):\n\u001b[1;32m     39\u001b[0m     n, c, h, w \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mshape\n\u001b[0;32m---> 40\u001b[0m     patches \u001b[39m=\u001b[39m patchify(images, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_patches)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m     tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_mapper(patches)\n\u001b[1;32m     43\u001b[0m     tokens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([torch\u001b[39m.\u001b[39mvstack((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_token,tokens[i])) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(tokens))])\n",
      "Cell \u001b[0;32mIn [3], line 13\u001b[0m, in \u001b[0;36mpatchify\u001b[0;34m(images, n_patches)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_patches):\n\u001b[1;32m     12\u001b[0m             patch \u001b[39m=\u001b[39m image[:, i \u001b[39m*\u001b[39m patch_size: (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m patch_size, j \u001b[39m*\u001b[39m patch_size: (j\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m patch_size]\n\u001b[0;32m---> 13\u001b[0m             patches[idx, i \u001b[39m*\u001b[39m n_patches \u001b[39m+\u001b[39m j] \u001b[39m=\u001b[39m patch\u001b[39m.\u001b[39;49mflatten()\n\u001b[1;32m     15\u001b[0m \u001b[39mreturn\u001b[39;00m patches\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
