{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/projects/sign_language/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 10:56:28.479289: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-11 10:56:31.188570: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-11 10:56:31.188914: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-11 10:56:31.188930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pytorchvideo.data\n",
    "\n",
    "import pathlib\n",
    "from transformers import VideoMAEFeatureExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/videomae/feature_extraction_videomae.py:28: FutureWarning: The class VideoMAEFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use VideoMAEImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_ckpt = \"MCG-NJU/videomae-base\"\n",
    "feature_extractor = VideoMAEFeatureExtractor.from_pretrained(model_ckpt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 32 32\n",
      "Total videos: 128\n",
      "64 32 32\n",
      "Total videos: 128\n",
      "64 32 32\n",
      "Total videos: 128\n"
     ]
    }
   ],
   "source": [
    "for fold in range(folds):\n",
    "    dataset_root_path = '/projects/ZHO/formats/letters_splitted/'+str(fold)\n",
    "    dataset_root_path = pathlib.Path(dataset_root_path)    \n",
    "    video_count_train = len(list(dataset_root_path.glob(\"train/*/*.mp4\")))\n",
    "    video_count_val = len(list(dataset_root_path.glob(\"val/*/*.mp4\")))\n",
    "    video_count_test = len(list(dataset_root_path.glob(\"test/*/*.mp4\")))\n",
    "    video_total = video_count_train + video_count_val + video_count_test\n",
    "    print(video_count_train, video_count_val, video_count_test)\n",
    "    print(f\"Total videos: {video_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_video_file_paths = (\n",
    "    list(dataset_root_path.glob(\"train/*/*.mp4\"))\n",
    "    + list(dataset_root_path.glob(\"val/*/*.mp4\"))\n",
    "    + list(dataset_root_path.glob(\"test/*/*.mp4\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "\n",
    "mean = feature_extractor.image_mean\n",
    "std = feature_extractor.image_std\n",
    "resize_to = feature_extractor.size['shortest_edge']\n",
    "print(resize_to)\n",
    "num_frames_to_sample = 16#model.config.num_frames\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "#clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "clip_duration =10\n",
    "print(clip_duration)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training dataset transformations.\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    #RandomShortSideScale(min_size=256, max_size=320),\n",
    "                    Resize((resize_to,resize_to)),\n",
    "                    #RandomHorizontalFlip(p=0.5),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Validation and evaluation datasets' transformations.\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize((resize_to, resize_to)),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "# Training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 11]\n",
      "[ 4 26]\n",
      "[28 25]\n",
      "[19 16]\n",
      "[ 9 24]\n",
      "[ 7 25]\n",
      "[29 23]\n",
      "[28 26]\n",
      "[21  3]\n",
      "[23  2]\n",
      "[30 27]\n",
      "[20 21]\n",
      "[15 22]\n",
      "[18 17]\n",
      "[27  8]\n",
      "[7 1]\n",
      "[31  5]\n",
      "[11  2]\n",
      "[16 13]\n",
      "[24  4]\n",
      "[6 8]\n",
      "[19 17]\n",
      "[ 0 30]\n",
      "[14  9]\n",
      "[ 0 18]\n",
      "[15  5]\n",
      "[1 6]\n",
      "[ 3 10]\n",
      "[14 22]\n",
      "[31 29]\n",
      "[20 10]\n",
      "[13 12]\n",
      "[7]\n",
      "[19]\n",
      "[26]\n",
      "[5]\n",
      "[17]\n",
      "[24]\n",
      "[23]\n",
      "[9]\n",
      "[12]\n",
      "[16]\n",
      "[18]\n",
      "[10]\n",
      "[4]\n",
      "[0]\n",
      "[3]\n",
      "[25]\n",
      "[28]\n",
      "[20]\n",
      "[21]\n",
      "[8]\n",
      "[6]\n",
      "[1]\n",
      "[11]\n",
      "[15]\n",
      "[14]\n",
      "[27]\n",
      "[30]\n",
      "[29]\n",
      "[31]\n",
      "[2]\n",
      "[22]\n",
      "[13]\n",
      "[22  9]\n",
      "[21 12]\n",
      "[12  1]\n",
      "[23 14]\n",
      "[19  2]\n",
      "[25 21]\n",
      "[ 2 29]\n",
      "[18 14]\n",
      "[31  5]\n",
      "[25 26]\n",
      "[30  5]\n",
      "[20 13]\n",
      "[ 8 19]\n",
      "[29  4]\n",
      "[23 16]\n",
      "[20  0]\n",
      "[10 13]\n",
      "[16  6]\n",
      "[18  3]\n",
      "[ 7 27]\n",
      "[28 28]\n",
      "[ 4 15]\n",
      "[ 0 17]\n",
      "[11 10]\n",
      "[30  8]\n",
      "[27 24]\n",
      "[11 26]\n",
      "[1 3]\n",
      "[ 6 22]\n",
      "[ 7 17]\n",
      "[15 24]\n",
      "[ 9 31]\n",
      "[26]\n",
      "[13]\n",
      "[0]\n",
      "[22]\n",
      "[7]\n",
      "[6]\n",
      "[29]\n",
      "[10]\n",
      "[19]\n",
      "[28]\n",
      "[30]\n",
      "[20]\n",
      "[18]\n",
      "[2]\n",
      "[4]\n",
      "[5]\n",
      "[1]\n",
      "[23]\n",
      "[3]\n",
      "[11]\n",
      "[9]\n",
      "[8]\n",
      "[14]\n",
      "[17]\n",
      "[25]\n",
      "[31]\n",
      "[16]\n",
      "[15]\n",
      "[21]\n",
      "[12]\n",
      "[24]\n",
      "[27]\n",
      "[31  4]\n",
      "[17  9]\n",
      "[26 16]\n",
      "[25 18]\n",
      "[2 8]\n",
      "[13 28]\n",
      "[29  9]\n",
      "[5 6]\n",
      "[18  1]\n",
      "[15 15]\n",
      "[ 8 13]\n",
      "[ 6 28]\n",
      "[ 0 25]\n",
      "[31  4]\n",
      "[ 7 11]\n",
      "[ 2 21]\n",
      "[20 30]\n",
      "[ 5 20]\n",
      "[24 19]\n",
      "[22 14]\n",
      "[12 27]\n",
      "[ 3 29]\n",
      "[19  7]\n",
      "[ 0 23]\n",
      "[21 14]\n",
      "[24 12]\n",
      "[ 1 22]\n",
      "[10 17]\n",
      "[10 26]\n",
      "[ 3 23]\n",
      "[30 16]\n",
      "[11 27]\n",
      "[18]\n",
      "[7]\n",
      "[28]\n",
      "[19]\n",
      "[27]\n",
      "[23]\n",
      "[29]\n",
      "[14]\n",
      "[26]\n",
      "[1]\n",
      "[30]\n",
      "[9]\n",
      "[8]\n",
      "[25]\n",
      "[12]\n",
      "[0]\n",
      "[31]\n",
      "[6]\n",
      "[20]\n",
      "[15]\n",
      "[17]\n",
      "[11]\n",
      "[3]\n",
      "[4]\n",
      "[21]\n",
      "[10]\n",
      "[13]\n",
      "[2]\n",
      "[5]\n",
      "[16]\n",
      "[22]\n",
      "[24]\n"
     ]
    }
   ],
   "source": [
    "folds = ['0','1','2']\n",
    "for fold in folds:\n",
    "    dataset_root_path = '/projects/ZHO/formats/letters_splitted/'+fold\n",
    "    dataset_root_path = pathlib.Path(dataset_root_path)\n",
    "    train_dataset = pytorchvideo.data.labeled_video_dataset(\n",
    "        data_path=os.path.join(dataset_root_path, \"train\"),\n",
    "        #data_path = os.path.join(dataset_root_path,'annotations','trainlist01.txt'),\n",
    "        clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "        decode_audio=False,\n",
    "        transform=train_transform,\n",
    "    )\n",
    "\n",
    "    test_dataset = pytorchvideo.data.labeled_video_dataset(\n",
    "        data_path=os.path.join(dataset_root_path, \"test\"),\n",
    "        #data_path = os.path.join(dataset_root_path,'annotations','trainlist01.txt'),\n",
    "        clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "        decode_audio=False,\n",
    "        transform=val_transform,\n",
    "    )\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size=32,shuffle = False, num_workers=32)  \n",
    "    test_dataloader = DataLoader(test_dataset,batch_size=32,num_workers=32)  \n",
    "\n",
    "    big_tensor_video_train = np.zeros((64,3,16,224,224))\n",
    "    #big_tensor_label_train = {}\n",
    "    big_tensor_video_train_list = {k:[] for k in range(32)}\n",
    "\n",
    "    big_tensor_video_test = np.zeros((32,3,16,224,224))\n",
    "    #big_tensor_label_test = []\n",
    "\n",
    "\n",
    "    for i,j in enumerate(train_dataloader):\n",
    "\n",
    "    #    input = j['video'].permute(0,2,1,3,4)\n",
    "\n",
    "        input = j['video'].numpy()\n",
    "        labels = j['label'].numpy()\n",
    "\n",
    "        for k in range(input.shape[0]):\n",
    "\n",
    "            big_tensor_video_train_list[labels[k]].append(input[k])\n",
    "        \n",
    "        #big_tensor_video_train.append(input)    \n",
    "        print(labels)\n",
    "\n",
    "    for k in range(len(big_tensor_video_train_list)):\n",
    "        for jj in range(2):\n",
    "            big_tensor_video_train[2*k+jj]=big_tensor_video_train_list[k][jj]\n",
    "\n",
    "    for i,j in enumerate(test_dataloader):\n",
    "    #    input = j['video'].permute(0,2,1,3,4)\n",
    "\n",
    "        input = j['video'].numpy()\n",
    "        labels = j['label'].numpy()\n",
    "\n",
    "        for k in range(input.shape[0]):\n",
    "\n",
    "            big_tensor_video_test[labels[k]] = input[k]\n",
    "        print(labels)\n",
    "\n",
    "    big_tensor_video_train_reshaped = np.transpose(big_tensor_video_train, (0,2,1,3,4))\n",
    "    big_tensor_video_test_reshaped  = np.transpose(big_tensor_video_test, (0,2,1,3,4))\n",
    "\n",
    "    np.save('train_video_letters_only_data_'+fold+'.npy',big_tensor_video_train_reshaped)\n",
    "    np.save('test_video_letters_only_data'+fold+'.npy',big_tensor_video_test_reshaped)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
